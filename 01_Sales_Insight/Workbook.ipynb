{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used Car Price Prediction\n",
    "\n",
    "### The Problem:\n",
    "\n",
    "-The price set for a car shouldnt be higher than other cars.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "-Data is everywhere.\n",
    "-Data analysis/science helps answer questions from the data. \n",
    "-Helps predicting the future.\n",
    "\n",
    "###  Python Libraries\n",
    "1. Scientific computing libraries\n",
    "    -Pandas -Data structures and tools\n",
    "    -NumPy - Arrays and Matrices\n",
    "    -SciPy - Advanced Math and visualization\n",
    "2. Visualization libraries\n",
    "    -Matplotlib - plots and graphs\n",
    "    -Seaborn - based on matplotlib\n",
    "3. Algorithms \n",
    "    -Scikit-learn - regression, classification, clustering\n",
    "    -Statsmodels - estimate stastical models,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Exporting data in Python\n",
    "\n",
    "1.Data acquistion\n",
    "    - Process of loading and reading data into Python from various resources.\n",
    "2. \n",
    "Data Format - Read - Save \n",
    "csv - pd.read_csv() - df.to_csv()\n",
    "json - pd.read_json() -df.to_json()\n",
    "Excel - pd.read_excel() - df.to_excel()\n",
    "sql - pd.read_sql() - df.to_sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Data in Python\n",
    "\n",
    "Using Pandas,we should check:\n",
    "1. Data Types\n",
    "2. Data Distribution \n",
    "\n",
    "By doing this, we can locate potential issues with the data.\n",
    "\n",
    "The different Insgights of Dataset, are:\n",
    "Pandas Type = Native Python type = Description\n",
    "object - string - numbers and strings\n",
    "int64 - int - numeric characters\n",
    "float64 - float - numeric characters\n",
    "datetime64, timedelta[ns] - - Time data \n",
    "\n",
    "Reasons to check data types:\n",
    "1. Potenial info and type mismatch\n",
    "2. Helps in using compatibility and python methods.\n",
    "\n",
    "### Basic Insights of Dataset\n",
    "\n",
    "df.dtypes - to check the data types\n",
    "df.describe() - to run a statistical summary of the dataset\n",
    "    -count, mean,std deviation, minimum value,25 percentile, 50 percentile , 75%, max\n",
    "df.describe(include = \"all\") - to get a full summary statistics, inclusing obejct types.\n",
    "    - unique - number of distinct values in the column\n",
    "    -top - frequently occuring object\n",
    "    -freq - number of times \n",
    "    NaN - not a number.\n",
    "df.info() \n",
    "    - displays top 30 and bottom 30 rows of the dataframe.\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Data in Python \n",
    "\n",
    "Data Pre-processing: Process of converting or mapping data from the initial \"raw\" form into another format, to prepare the data for further analysis.\n",
    "Also, known as: Data cleaning, data wrangling\n",
    "\n",
    "#### 1. Identify and handle missing values.\n",
    "#### 2. Data formatting \n",
    "#### 3. Data normalization (centering/scaling)\n",
    "#### 4. Data bining - creating categories to compare data\n",
    "#### 5. Categorical values to Numerical Values\n",
    "\n",
    "#### 1. Identify and handling of missing data:\n",
    "\n",
    "a) Each operation is done by columns.\n",
    "df['symbolizing']\n",
    "b) Dealing with missing values in a dataset\n",
    "    -missing values are represented as N/A, or 0 or blank cell.\n",
    "d) How to deal with missing data?\n",
    "    - Check with the data collection source.\n",
    "    - Remove the missing data ( the whole variable, or drop the data entry )\n",
    "    - Replacing Data is preferred \n",
    "        - Replace with an average (similar datapoints)\n",
    "        - For categorical values, one possibility is to replace with the frequently used values. \n",
    "    - Leave it as a mising data\n",
    "\n",
    "d) How to drop missing values in Python\n",
    "    - Use dataframes.dropna():\n",
    "        -axis = 0 - drop the entire row\n",
    "        -axis = 1 - drop the entire column\n",
    "        -eg: df.dropna(subset = [\"price\"], axis = 0, inplace = True). By having the argument \"inplace = True\", it allows to \n",
    "             modify the dataframe directly. \n",
    "             This is equivalent to writing: df = df.dropna(subset = [\"price\"], axis = 0)<br>\n",
    "             Documentation source: http://pandas.pydata.org\n",
    "\n",
    "e) How to replace missing values in Python\n",
    "    - Use dataframe.replace(missing_value, new_value):\n",
    "        eg: mean = df[\"normalized-losses\"].mean()<br>\n",
    "            df[\"normalized-losses\"].replace(np.nan,mean)<br>\n",
    "\n",
    "     - Leave the missing data as is\n",
    "\n",
    "#### 2. Data Formatting\n",
    "\n",
    "a) Bringing data into a common standard of expression allows users to make meaningful comparisons.\n",
    "    eg: Applying calculations to an entire column\n",
    "        df[\"city-mpg\"] = 235/df[\"city-mpg\"]<br>\n",
    "        \n",
    "        - df.rename(columns = {\"city_mpg\": \"city-L/100km\"}, inplace = True) <br>\n",
    "        \n",
    "b) Fixing incorrect data types\n",
    "    - A wrong data type is assigned to a feature. \n",
    "  -  Objects: \"A\", \"ENGINE\",...\n",
    "  -    int64: 1,3,5\n",
    "    - float64: 3.254, 12.422\n",
    "To identify data types:<br>\n",
    "    dataframe.dtypes()<br>\n",
    "To convert data types:<br>\n",
    "    datafrane.astype()<br>\n",
    "    eg: df[\"price'] = df[[\"price'].astype(\"int\")<br>\n",
    "    \n",
    "#### 3. Data Normalization\n",
    "\n",
    "a) Normalization is done to ensure the range of the values between the different features is consistent.\n",
    "    eg: Age and Income<br>\n",
    "        Age: 0-100<br>\n",
    "        Income: 20,000 - Higher. <br>\n",
    "        -Income is 1000 times greater than age values. <br>\n",
    "   Approaches for Normalization: <br>\n",
    "   1. Simple Feature scaling\n",
    "       - dividing the feature by the max value <br>\n",
    "       x_new = x_old / x_max<br>\n",
    "       df[\"length\"] = df[\"length\"]/df[\"length\"].max()<br>\n",
    "       \n",
    "   2. Min-Max \n",
    "       - \n",
    "       x_new = (x_old - x_min)/(x_max - x_min)<br>\n",
    "       df[\"length\"] = (df[\"length\"] - df[\"length\"].min())/<br>\n",
    "                       (df[\"length\"].max() - df[\"length\"].min())<br>\n",
    "                       \n",
    "   3. Z-score: Standard score\n",
    "       -subtract by avg and divide by std deviation.\n",
    "       x_new = x_old - mean / std-deviation<br>\n",
    "       df[\"length\"] = (df[\"length\"] - df[\"length\"].mean())/df[\"length\"].std() <br>\n",
    "       \n",
    "#### 4.Data Binning\n",
    "\n",
    "a) Binning: Grouping of values into \"bins\"\n",
    "    eg: Age: 1-5, 6-10, 11-15, etc.,\n",
    "    -categorization of numeric to categorical values\n",
    "    -group set of numerical values into a set of bins.\n",
    "    eg: price : 5000 to 45,5000<br>\n",
    "      bins: low:5000,10000,12000<br>\n",
    "            med: 30000,31000<br>\n",
    "            high: 39000,44000,44500<br>\n",
    "    -Binning in python pandas<br>\n",
    "     bins - np.linspace(min(df[\"price\"]),max(df[\"price\"]),4) <br>\n",
    "     group_names = [\"Low\", \"Medium\",\"High\"]<br>\n",
    "     df[\"price-binned\"] = pd.cut(df[\"prices\"], bins, label = group_names, include_lowest = True)<br>\n",
    "     -Visualize the bins into histograms.\n",
    "     \n",
    "#### 5. Turning categorical variables into quantitaive variables in Python\n",
    "\n",
    "a) Statistical/alogrithm models cannot take obejcts/strings as inputs <br>\n",
    "    - For better analysis, we categorize the string values as present/not present tp 1 or 0, as needed. <br>\n",
    "    - this is called one hot coding <br>\n",
    "    In Pandas, we can convert categorical variables to dummy variables(0 or 1)<br>\n",
    "    \n",
    "   eg: pd.get_dummies(df['fuel']) - gets the fuel type categories. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "### Preliminary steps:\n",
    "- Summarize main characteristics of the data.\n",
    "- Gain better understanding of the data.\n",
    "- Uncover relationships between variables.\n",
    "- Extract important variables.\n",
    "\n",
    "Question: What are the characteristics (features) that have the most impact on the car price (traget) ?\n",
    "\n",
    "#### 1. Descriptive statistics\n",
    "#### 2. Group By\n",
    "#### 3. ANOVA\n",
    "#### 4. Correlation\n",
    "#### 5. Correlation - Statistics\n",
    "\n",
    "\n",
    "#### 1. Descriptive Statistics\n",
    "- Basic features of the data.\n",
    "- Short summaries about the sample and measures of the date.\n",
    "a) df.describe() <br>\n",
    "b) value_counts() - for categorical data <br>\n",
    "c) Box Plots - median, upper quartile(75%), lower quartile(25%), interquartile range, upper and lower extreme ( 1.5 times above upper quartile), and outlier/single data. <br>\n",
    "    sns.boxplot(x=\"feature name \", y = \"target\", data = df)<br>\n",
    "d) Scatter Plot - relationship between two variables <br>\n",
    "    - predictor/independent vairables on X-axis.<br>\n",
    "    - trager/depenedent variables on Y-axis.<br>\n",
    "    eg: y = df[\"engine-size\"]<br>\n",
    "        x = df[\"price\"]<br>\n",
    "        plt.scatter(x,y)<br>\n",
    "        \n",
    "        plt.title(\"Scatterplot pf Engine size vs price\")<br>\n",
    "        plt.xlabel(\"Engine Size\")<br>\n",
    "        plt.ylabel(\"Price\")<br>\n",
    "        - helps establish if a linear relationship exists.<br>\n",
    "\n",
    "#### 2. GroupBy\n",
    "- Basics of grouping.\n",
    "- Helps in identifying the features that impact on the target variable.\n",
    "a) dataframe.Groupby() <br>\n",
    "-can be applied to categorical variables.<br>\n",
    "-group data into categories.<br>\n",
    "-single or multiple variables.<br>\n",
    "\n",
    "eg: df_test = df[[\"drive-wheel\", \"body-style\",\"price\"]]<br>\n",
    "    df_grp = df_test.groupby([\"drive-wheels\", \"body-style\"], as_index = False).mean()<br>\n",
    "    When viewing the data, we can arrive at some conclusions. <br>\n",
    "##### Using the pivot table, it helps us visualize better. \n",
    "    df_pivot = df_grp.pivot(index = 'drive-wheel', columns = 'body-style')<br>\n",
    "##### Heatmap, plot target variables over multiple variables\n",
    "    plt.pcolor(df_pivot, cmap = 'RdBu')<br>\n",
    "    plt.colorbar()<br>\n",
    "    plt.show()<br>\n",
    "\n",
    "#### 3. Analysis of Variance ANOVA (statsitical test)\n",
    "   - Statistical comparison of groups<br>\n",
    "    eg: average price of different vehicle makes<br>\n",
    "   - ANOVA can be done to find correlation between different groups of a categorical variable.<br>\n",
    "    Results from ANOVA:<br>\n",
    "   - F-test score:variation between sample group means divided by variation within sample groups<br>\n",
    "       Small F value implies poor correlation between variable categories and target variables. <br>\n",
    "       Large F value implies strong correlation between variable categoeis and target variables.<br>\n",
    "   - p-value: confidence degree, lets us know if the obtained value is statistically correct.<br>\n",
    "        Small p-value, the relation is correct.<br>\n",
    "        Large p-value, the relation is weak.<br>\n",
    "    \n",
    "   - Using the scipy package, we can perform the annova test to retrieve the correlation between variable categories.<br>\n",
    "        df_anova = df[[\"make\",\"price\"]]<br>\n",
    "        groupe_annova = df_anova.groupby([\"make\"])<br>\n",
    "    \n",
    "        anova_results_1 = stats.f_oneway(grouped_anova.get_group(\"honda\")[\"price\"], grouped_anova.get_group(\"subaru\")[\"price\"])<br>\n",
    "\n",
    "#### 4. Correlation\n",
    "- Measures to what extent different variables are independent.<br>\n",
    "    eg: lung cance -> smoking.<br>\n",
    "    \n",
    "    -Correlation does not imply causation.<br>\n",
    "- a) Correlation: Positive Linear relationship<br>\n",
    "       -correlation between two features ( engine-size and price ) <br>\n",
    "   \n",
    "       sns.regplot( x = \"engine-size\", y = \"price\", data = df)<br>\n",
    "       plt.ylim(0,)<br>\n",
    "       The main goal is to view the engine size impact on the price. <br>\n",
    "   \n",
    "- b) Correlation: Negative Linear relationship<br>\n",
    "    - correlation between two features ( highway-mpg and price)<br>\n",
    "        sns.regplot( x = \"highway-mpg\", y = \"price\". data = df)<br>\n",
    "        plt.ylim(0,)<br>\n",
    "         When highway miles price gallon is low, the price is low.<br>\n",
    "        This gives a negative linear relation with the car price. <br>\n",
    "    \n",
    "- c) Correlation: Weak relationship<br>\n",
    "    - correlation between two features is weak ( peak-rpm and price )<br>\n",
    "         sns.regplot(x =\"peak-rpm\", y = \"price\", data = df)<br>\n",
    "        plt.ylim(0,)<br>\n",
    "        It is recommended not to use this variable(feature) to predict the values.<br>\n",
    "    \n",
    "#### 5. Correlation : Statistical Methods\n",
    "\n",
    "a) Pearson correlation: It measures the strength of the correlation between two features.<br>\n",
    "   - correlation coefficient<br>\n",
    "    - p-value<br>\n",
    "    \n",
    "   - Correlation coefficient:<br>\n",
    "        Close to +1: Large +ve relationship<br>\n",
    "        Close to -1: Large -ve relationship<br>\n",
    "        Close to 0:  No relationship<br>\n",
    "    \n",
    "   - P-Value<br>\n",
    "        p-value <0.001:  Strong certainity in the result<br>\n",
    "        p-value <0.05:   Moderate certainity in the result<br>\n",
    "        p-value <0.1:    Weak certainity in the result<br>\n",
    "        p-value >0.1:    No certainity in the result <br>\n",
    "    \n",
    "        eg: pearson_coef, p_value = stats.pearsonr(df[\"horsepower\"], df['price'])<br>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development\n",
    "Model is a mathematical equation used to predict a value given one or more, other values.\n",
    "Relating one or more independent variables to dependent variables.\n",
    "\n",
    "Eg: Input -> Independent variables or features (highway-mpg) gives the -> dependent variables (predicted price)\n",
    "\n",
    "#### 1. Simple and Multiple Linear Regression\n",
    "\n",
    "- Linear regression refers to one independent variable to make a prediction.\n",
    "- Multiple Linear regression will refer to multiple independent variables to make a prediction.\n",
    "\n",
    "##### SLR: Single Linear Regression\n",
    "- Predictor(independent) variable - x\n",
    "- Target (depenedent) variable - y\n",
    "\n",
    "        y = b0 + b1x\n",
    "\n",
    "- Predicting the Simple linear regression \n",
    "\n",
    "- Steps to follow:\n",
    "    - import linear model from scikit learn\n",
    "\n",
    "##### from sklearn.linear_model import LinearRegression\n",
    "- b) create a linear regression object using the constructor\n",
    "#####  lm = LinearRegression()\n",
    "\n",
    "\n",
    "- c) Fitting a simple linear model \n",
    "    - define the predictor variable and target variable\n",
    "    X = df[['highway-mpg']]\n",
    "    Y = df['price']\n",
    "\n",
    "- d) then, use lm.fit(X,Y) to fit the model, i.e., fit the parameteres b0 and b1\n",
    "__lm.fit(X,Y)__\n",
    "\n",
    "- e) We obtain the prediction\n",
    "    __Yhat = lm.predict(X)__\n",
    "\n",
    "- f) Yhat = b0 + b1*x\n",
    "        Here, the intercept value is the b0\n",
    "        the slope value is the b1, \n",
    "        - The relationship between the Price and Highway MPG can be given by:\n",
    "        Price = b0 value + b1 value * x (the highway mpg value)\n",
    "\n",
    "##### MLR: Multiple Linear Regression\n",
    "This explains the relationship between:\n",
    "- one continuous target(Y) variable\n",
    "- two or more predictor(X) variables\n",
    "\n",
    "    eg: yhat = b0 + b1x1 + b2x2 + b3x3 + b4x4\n",
    "\n",
    "    b0: intercept(x=0)\n",
    "    b1: coffecient of parament x1 and so on...\n",
    "\n",
    "- Fitting a multiple linear model estimator ( 4 variables)\n",
    "    - extracting the 4 predictor variables and storing them in the variable Z\n",
    "        z = df[['hp', 'curb -wt', 'engine-size', 'highway-mpg']]\n",
    "\n",
    "- Traing the model as before\n",
    "    lm.fit(z, df['price'])\n",
    "\n",
    "        We get a prediction\n",
    "        Yhat = lm.predict(X)\n",
    "\n",
    "##### Model evaluation using Visualization\n",
    "- a) Regression plots help us visualize\n",
    "    - relationship between two variables\n",
    "    - strenght of the correlation\n",
    "    - direction of the relationship (positive or negative)\n",
    "    \n",
    "- b)  Regression plot is a combination of:\n",
    "    - scatterplot: each point represents a different y value.\n",
    "    - x- axis is the independnet variable.\n",
    "    - y- axis is the dependent variable.\n",
    "    -the fitted line represents the predicted value.\n",
    "    \n",
    "- c) Plotting of regression\n",
    "- import seaborn as sns\n",
    " \n",
    "     sns.regplot(x = \"highway-mpg\" (alwasys the feature), y = 'price'(the target variable), data = df)\n",
    "     plt.ylim(0,)\n",
    " \n",
    "- d) Residual Plot reps the error between the actual target value and predicted target value.\n",
    "\n",
    "    - When viewing the Residual plot, the points are randomly spread out around x-axis then a linear model is appropriate. \n",
    "    - If there is a curvature, the values of the error change with x. So, this suggests the linear assumption is incorrect. \n",
    "    - If the variance( the difference between the largest delta and the smallest delta( largest -ve delta) increases with x, the linear assumption is incorrect. \n",
    "\n",
    "__Plotting a residual plot.__\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "- sns.residplot(df['highway-mpg'], df['price'])\n",
    " the first parameter is a series of independent varaible, the features. <br>\n",
    " the second parameter is a series of depenedent vairable, the target.<br>\n",
    " \n",
    "- the distribution plot helps visualize the predicted value against the actual value\n",
    "    Pandas will plot a distribution plot.\n",
    "\n",
    "- Code for distribution plot\n",
    "    import seaborn as sns\n",
    "    ax1 = sns.distplot(df['prices'], hist = False, color = 'r', label = 'Actual value)\n",
    "\n",
    "    sns.distplot(Yhat, hist = False, color = \"b\", label = 'Fitted Values', ax = ax1) \n",
    "\n",
    "#### Measures for In-sample evaluation\n",
    "Numerically evaluating our models to determine how good it fits on the dataset. <br>\n",
    "\n",
    "The two important measures to determine the fit of a model are:<br>\n",
    "- Mean Sqaured error (MSE)\n",
    "- R squared\n",
    "\n",
    "##### MSE \n",
    "from sklearn.metrics mean_squared_error<br>\n",
    "\n",
    "mean_squared_error(df['price'], Y_predic_simple_fit)\n",
    "\n",
    "##### R-squared R^2\n",
    "- it is a measure to determine how close the data is to the fitted regression line\n",
    "    R^2, the percentage of variation of the target variable(Y)\n",
    "\n",
    "- Coefficient of determination(R^2)\n",
    "    R^2 = (1 - (MSE of reg line/MSE of avg data))\n",
    "\n",
    "- If the value is near 1, it is a good fit. \n",
    "    lm.score(X,y)-- gives the r squared value.\n",
    "\n",
    "#### Prediction and Decision Making\n",
    "To determine the final best fit, we need to look at a combination of:\n",
    "- Do the predicted values make sense\n",
    "- Visualization\n",
    "- Numerical measures for evaluation\n",
    "- Comparing models\n",
    "\n",
    "##### Analysis on the predicted values\n",
    "- First we train the model \n",
    "    lm.fit(df['highway-mpg'], df['prices']))\n",
    "- Let's predict the price of a car with 30 highway mpg\n",
    "    lm.predict(30)\n",
    "    -We can examine the coefficients by using lm.coef_\n",
    "- To predict values that make sense\n",
    "import numpy as np\n",
    "\n",
    "- using the numpy function, we arrange to generate a sequence from 1 to 100.\n",
    "new_input = np.arange(1,101,1).reshape(-1,1)\n",
    "\n",
    "- Using the output to predict new values \n",
    "yhat = lm.predict(new_input)\n",
    "The output is a numpy array\n",
    "\n",
    "- Next, visualize the data with a regression plot\n",
    "    the data trends down.\n",
    "- Then, the residual plot \n",
    "- We might observe a curvature, which is not desirable.\n",
    "A Distribution plot will help visualize better<br>\n",
    "- the Mean squared error is more efficient in helping us view the accuracy of the model.\n",
    "Comparing MLR AND SLR- multiple linear reg and single linear reg<br>\n",
    "- a Lower MSE does not imply a better fit.\n",
    "- MSE(mean squared error) for a MLR model will smaller than the MSE for a SLR model, since the errors of the data will decrease when more variables are included in the model.\n",
    "- Polynomial reg will have a smaller MSE than a regular reg model.\n",
    "\n",
    "#### Model Evaluation and Refinement\n",
    "\n",
    "- We can evaluate our model to test how well it performs in real world scenarios.\n",
    "- We can achieve this by:\n",
    "    - In sample data or training data.\n",
    "    - Out of sample evaluation or test data.\n",
    "    \n",
    "- Training/Testing sets \n",
    "    For Training,we use 70% of the data set<br>\n",
    "    For Testting,we use 30% of the data set<br>\n",
    "\n",
    "- We use the training set, to build and train the model with a training set.\n",
    "- We use the testing set to assess the performance of a predictive model.\n",
    "\n",
    "Function train_test_split()\n",
    "\n",
    "###### from sklearn.model_selection import train_test_split \n",
    "\n",
    "    x_train,x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.3, random_state = 0)\n",
    "\n",
    "    x_data: features or independnet variables\n",
    "    y_data: dataset target: df['price']\n",
    "    x_train, y_train = parts of avaialble data as training set\n",
    "    x_test, y_test = parts of available data as testing set\n",
    "\n",
    "Generalization Performance\n",
    "- measure of how well our data does at predicting previously unseen data.\n",
    "- the error we obtain using our testing data is an approximation of this error.\n",
    "\n",
    "Cross Validation\n",
    "- each observation of the  varied training and testing set is used effectively cross validate the model.\n",
    "- 70% training and 30% test.\n",
    "- 30% test and 70% training. \n",
    "\n",
    "- Function cross_val_score()<br>\n",
    "    from sklearn.model_selection import cross_val_score<br>\n",
    "\n",
    "    scores = cross_val_score( lr, x_data, y_data, cv = 3)<br>\n",
    "    lr = linear regression model<br>\n",
    "    x_data - predictor variable<br>\n",
    "    y_data - target variable <br>\n",
    "    cv = 3  partition.\n",
    "\n",
    "np.mean(scores) - to get the mean of the scores.<br>\n",
    "\n",
    "- Here, the data set is split into 70/30 in 3 different combinations (slices)<br>\n",
    "    For every combination, a unique 70/30 training/test is used and a r^2 value of it is stored in a separte array.<br>\n",
    "    In this same way, the r^2 value is calculated for the other two combinations.<br>\n",
    "    The mean of the r^2 value is calculated. <br>\n",
    "    \n",
    "    Function cross_val_predict() - gives the predicted value that is stored in the array, before the r^2 value is calculated. <br>\n",
    "\n",
    "#### Overfitting, underfitting and model Selection\n",
    "For example, we have a function and the training point comes from a polynomial function. <br>\n",
    "The goal of a model selection is to determine the order of the polynomial. <br>\n",
    "- We try a linear order and this would exhibit underfitting \n",
    "- We try the eighth order and this can exhibit a closely related model. ~ more accurate. \n",
    "- We try the sixteenth order and this exhibits an overly fitted model. where the model is too flexible \n",
    "\n",
    "- The model selection can be visualized using a plot, x - order of the function. y - error mse. \n",
    "- The eighth order fucntion can display some error and this error is very less compared to the error of the linear model or 16th order model. the test error for the 8th order function would be suitable.\n",
    "\n",
    "We can test this out using R^2:\n",
    "    Rsqu_test = []\n",
    "    order = [1,2,3,4]\n",
    "    for n in order:\n",
    "        pr = polynomialFeatures(degree = n)\n",
    "        x_train_pr = pr.fit_transform(x_train[['horsepower']])\n",
    "        x_test_pr = pr.fit_transform(x_test[['horsepower']])\n",
    "        lr.fit(x_train)pr, y_train)\n",
    "        Rsqu_test.append(lr.score(x_test-pr, y_test)) \n",
    "    \n",
    "    \n",
    "#### Ridge Regression\n",
    "We use alpha, control values we enter to control the higher order function.\n",
    "\n",
    "#### Grid Search\n",
    "- aplha is a hyper-parameter (we can get it by cross,validation) \n",
    "    We use different hyper paramters, to get different Model. \n",
    "    We select the model, based on its error. \n",
    "    We train/test the data again to get the most accurate r^2 error. \n",
    "    Grid search\n",
    "        -parameters= [{'alpha': [1,10,100,1000]}]\n",
    "        Ridge()\n",
    "        Grid Search CV \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
